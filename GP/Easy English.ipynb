{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Datasets Loading for Training and Testing:\n",
    "\n",
    "We have 2 Models:\n",
    "    -A Model for Converting Audio Signals to a Sequence of Character.\n",
    "    -A Model for Converting a Sequence of Characters to Syllables.\n",
    "Our First Model is trained using LibriSpeech Loaded for PyTorch DataLoader.\n",
    "The Second Model is trained using the CMU Pronunciation Dictionary Loaded From Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samer\\Anaconda3\\lib\\site-packages\\torchaudio\\extension\\extension.py:13: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\samer\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\samer\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\samer\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\samer\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\samer\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\samer\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Multiply, Reshape, RepeatVector, Lambda, Flatten\n",
    "from keras.activations import softmax\n",
    "from keras.layers import Embedding, Dropout, Activation\n",
    "from fast_ctc_decode import viterbi_search\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as K\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "\n",
    "##Load LibriSpeech\n",
    "if not os.path.isdir(\"./data\"):\n",
    "    os.makedirs(\"./data\")\n",
    "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"test-clean\", download=True)\n",
    "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"train-clean-100\", download=True)\n",
    "else:\n",
    "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"test-clean\", download=False)\n",
    "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"train-clean-100\", download=False)\n",
    "\n",
    "####################################################################################################################################\n",
    "\n",
    "##Load CMU Pronunciation Dictionary \n",
    "CMU_DICT_PATH = os.path.join(\n",
    "    'GRAPHEME_TO_PHONEME/input/cmu-pronunciation-dictionary-unmodified-07b/cmudict-0.7b')\n",
    "CMU_SYMBOLS_PATH = os.path.join(\n",
    "    'GRAPHEME_TO_PHONEME/input/cmu-pronouncing-dictionary/cmudict.symbols')\n",
    "##Remove words with numbers or symbols\n",
    "ILLEGAL_CHAR_REGEX = \"[^A-Z-'.]\"\n",
    "##Setting limits for the words from the Dataset\n",
    "MAX_DICT_WORD_LEN = 20\n",
    "MIN_DICT_WORD_LEN = 2\n",
    "def load_clean_phonetic_dictionary():\n",
    "    def is_alternate_pho_spelling(word):\n",
    "        ##No word has more than 9 pronunciations to it is safe to use .isdigit() \n",
    "        ##Format alternative pronunciations as \"WORD(#)\"\n",
    "        return word[-1] == ')' and word[-3] == '(' and word[-2].isdigit() \n",
    "    ##Ignore words with symbols or numbers or that don't satisfy the set limits\n",
    "    def should_skip(word):\n",
    "        if not word[0].isalpha():\n",
    "            return True\n",
    "        ##Ignore abbreviations\n",
    "        if word[-1] == '.':  \n",
    "            return True\n",
    "        if re.search(ILLEGAL_CHAR_REGEX, word):\n",
    "            return True\n",
    "        if len(word) > MAX_DICT_WORD_LEN:\n",
    "            return True\n",
    "        if len(word) < MIN_DICT_WORD_LEN:\n",
    "            return True\n",
    "        return False\n",
    "    phonetic_dict = {}\n",
    "    ##Cleaning up the Dataset\n",
    "    with open(CMU_DICT_PATH, encoding=\"ISO-8859-1\") as cmu_dict:\n",
    "        for line in cmu_dict:\n",
    "            ##Skip comments\n",
    "            if line[0:3] == ';;;':\n",
    "                continue\n",
    "            word, phonetic = line.strip().split('  ')\n",
    "            # Alternate pronounciations are formatted: \"WORD(#)  PHONETICS\"\n",
    "            # Remove (#)\n",
    "            if is_alternate_pho_spelling(word):\n",
    "                word = word[:word.find('(')]\n",
    "            if should_skip(word):\n",
    "                continue\n",
    "\n",
    "            if word not in phonetic_dict:\n",
    "                phonetic_dict[word] = []\n",
    "            phonetic_dict[word].append(phonetic)\n",
    "    #phonetic_dict = {key:phonetic_dict[key] for key in random.sample(list(phonetic_dict.keys()), 5000)} #Limiting Dataset for Testing\n",
    "    return phonetic_dict\n",
    "phonetic_dict = load_clean_phonetic_dictionary()\n",
    "example_count = np.sum([len(prons) for _, prons in phonetic_dict.items()])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Data Preperation and Helper Functions for Training and Testing\n",
    "\n",
    "For the first model:\n",
    "    -We'll take raw audio waves and transform them into Mel Spectrograms.\n",
    "    -Each sample of the dataset contains the waveform, sample rate of audio, the utterance/label.\n",
    "    -Loaded Data Format: https://github.com/pytorch/audio/blob/master/torchaudio/datasets/librispeech.py#L40\n",
    "    -We apply Spectrogram Augmentation by FrequencyMasking , and TimeMasking.\n",
    "    -Since we rely on Denoising our Input Data before predicting we didn't inject noises.\n",
    "    -We trained with the clean dataset of LibriSpeech this forced us to either Denoise before Predicition or Inject Noise.\n",
    "For the second model:\n",
    "    -We'll treat words as squences of characters and pronunciations as sequences of phoneme symbols.\n",
    "    -We assign each character and each phoneme a number. \n",
    "    -We convert these numbers to represent chars/phonemes as 1-hot vectors.\n",
    "    -We'll add start & end symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samer\\Anaconda3\\lib\\site-packages\\torchaudio\\functional\\functional.py:358: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  \"At least one mel filterbank has all zero values. \"\n"
     ]
    }
   ],
   "source": [
    "##LibriSpeech Model Data Preperation\n",
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
    "    def __init__(self):\n",
    "        char_map_str = \"\"\"\n",
    "        ' 0\n",
    "        <SPACE> 1\n",
    "        a 2\n",
    "        b 3\n",
    "        c 4\n",
    "        d 5\n",
    "        e 6\n",
    "        f 7\n",
    "        g 8\n",
    "        h 9\n",
    "        i 10\n",
    "        j 11\n",
    "        k 12\n",
    "        l 13\n",
    "        m 14\n",
    "        n 15\n",
    "        o 16\n",
    "        p 17\n",
    "        q 18\n",
    "        r 19\n",
    "        s 20\n",
    "        t 21\n",
    "        u 22\n",
    "        v 23\n",
    "        w 24\n",
    "        x 25\n",
    "        y 26\n",
    "        z 27\n",
    "        \"\"\"\n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        for line in char_map_str.strip().split('\\n'):\n",
    "            ch, index = line.split()\n",
    "            self.char_map[ch] = int(index)\n",
    "            self.index_map[int(index)] = ch\n",
    "        self.index_map[1] = ' '\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            if c == ' ':\n",
    "                ch = self.char_map['<SPACE>']\n",
    "            else:\n",
    "                ch = self.char_map[c]\n",
    "            int_sequence.append(ch)\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.index_map[i])\n",
    "        return ''.join(string).replace('<SPACE>', ' ')\n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    ")\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "text_transform = TextTransform()\n",
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, _, utterance, _, _, _) in data:\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == 'valid':\n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            raise Exception('data_type should be train or valid')\n",
    "        spectrograms.append(spec)\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "    return spectrograms, labels, input_lengths, label_lengths\n",
    "\n",
    "#################################################################################################################################\n",
    "\n",
    "##Data Preperation for CMU Phonetics Dictionary\n",
    "START_PHONE_SYM = '\\t'\n",
    "END_PHONE_SYM = '\\n'\n",
    "def char_list():\n",
    "    allowed_symbols = [\".\", \"-\", \"'\"]\n",
    "    uppercase_letters = list(string.ascii_uppercase)\n",
    "    return [''] + allowed_symbols + uppercase_letters\n",
    "def phone_list():\n",
    "    phone_list = [START_PHONE_SYM, END_PHONE_SYM]\n",
    "    with open(CMU_SYMBOLS_PATH) as file:\n",
    "        for line in file: \n",
    "            phone_list.append(line.strip())\n",
    "    return [''] + phone_list\n",
    "def id_mappings_from_list(str_list):\n",
    "    str_to_id = {s: i for i, s in enumerate(str_list)} \n",
    "    id_to_str = {i: s for i, s in enumerate(str_list)}\n",
    "    return str_to_id, id_to_str\n",
    "char_to_id, id_to_char = id_mappings_from_list(char_list())\n",
    "phone_to_id, id_to_phone = id_mappings_from_list(phone_list())\n",
    "CHAR_TOKEN_COUNT = len(char_to_id)\n",
    "PHONE_TOKEN_COUNT = len(phone_to_id)\n",
    "def char_to_1_hot(char):\n",
    "    char_id = char_to_id[char]\n",
    "    hot_vec = np.zeros((CHAR_TOKEN_COUNT))\n",
    "    hot_vec[char_id] = 1.\n",
    "    return hot_vec\n",
    "def phone_to_1_hot(phone):\n",
    "    phone_id = phone_to_id[phone]\n",
    "    hot_vec = np.zeros((PHONE_TOKEN_COUNT))\n",
    "    hot_vec[phone_id] = 1.\n",
    "    return hot_vec\n",
    "MAX_CHAR_SEQ_LEN = max([len(word) for word, _ in phonetic_dict.items()])\n",
    "MAX_PHONE_SEQ_LEN = max([max([len(pron.split()) for pron in pronuns]) \n",
    "                         for _, pronuns in phonetic_dict.items()]\n",
    "                       ) + 2  ## \"+ 2\" token Start and End\n",
    "def dataset_to_1_hot_tensors():\n",
    "    char_seqs = []\n",
    "    phone_seqs = []\n",
    "    for word, pronuns in phonetic_dict.items():\n",
    "        word_matrix = np.zeros((MAX_CHAR_SEQ_LEN, CHAR_TOKEN_COUNT))\n",
    "        for t, char in enumerate(word):\n",
    "            word_matrix[t, :] = char_to_1_hot(char)\n",
    "        for pronun in pronuns:\n",
    "            pronun_matrix = np.zeros((MAX_PHONE_SEQ_LEN, PHONE_TOKEN_COUNT))\n",
    "            phones = [START_PHONE_SYM] + pronun.split() + [END_PHONE_SYM]\n",
    "            for t, phone in enumerate(phones):\n",
    "                pronun_matrix[t,:] = phone_to_1_hot(phone)\n",
    "                \n",
    "            char_seqs.append(word_matrix)\n",
    "            phone_seqs.append(pronun_matrix)\n",
    "    \n",
    "    return np.array(char_seqs), np.array(phone_seqs)\n",
    "char_seq_matrix, phone_seq_matrix = dataset_to_1_hot_tensors()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test Helper Functions for the LibriSpeech Model\n",
    "def avg_wer(wer_scores, combined_ref_len):\n",
    "    return float(sum(wer_scores)) / float(combined_ref_len)\n",
    "##levenshtein disctance is defined as the minimum number of single-character edits (substitutions, insertions ordeletions) required to change one word into the other. \n",
    "##We can naturally extend the edits to word level when calculating levenshtein disctance for two sentences.\n",
    "def _levenshtein_distance(ref, hyp):\n",
    "    m = len(ref)\n",
    "    n = len(hyp)\n",
    "    ##Special Scenarios\n",
    "    if ref == hyp:\n",
    "        return 0\n",
    "    if m == 0:\n",
    "        return n\n",
    "    if n == 0:\n",
    "        return m\n",
    "    if m < n:\n",
    "        ref, hyp = hyp, ref\n",
    "        m, n = n, m\n",
    "    distance = np.zeros((2, n + 1), dtype=np.int32)\n",
    "    ##Set Initial Distances in Matrix\n",
    "    for j in range(0,n + 1):\n",
    "        distance[0][j] = j\n",
    "    ##Levenshtein Distance Algorithm\n",
    "    for i in range(1, m + 1):\n",
    "        prev_row_idx = (i - 1) % 2\n",
    "        cur_row_idx = i % 2\n",
    "        distance[cur_row_idx][0] = i\n",
    "        for j in range(1, n + 1):\n",
    "            if ref[i - 1] == hyp[j - 1]:\n",
    "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
    "            else:\n",
    "                s_num = distance[prev_row_idx][j - 1] + 1 ##Substitutions\n",
    "                i_num = distance[cur_row_idx][j - 1] + 1 ##Insertions\n",
    "                d_num = distance[prev_row_idx][j] + 1 ##Deletions\n",
    "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
    "    return distance[m % 2][n]\n",
    "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
    "    if ignore_case == True:\n",
    "        reference = reference.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "    ref_words = reference.split(delimiter)\n",
    "    hyp_words = hypothesis.split(delimiter)\n",
    "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
    "    return float(edit_distance), len(ref_words)\n",
    "def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n",
    "    if ignore_case == True:\n",
    "        reference = reference.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "    join_char = ' '\n",
    "    if remove_space == True:\n",
    "        join_char = ''\n",
    "    reference = join_char.join(filter(None, reference.split(' ')))\n",
    "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
    "    edit_distance = _levenshtein_distance(reference, hypothesis)\n",
    "    return float(edit_distance), len(reference)\n",
    "##WER = (Sw + Dw + Iw) / Nw\n",
    "##Sw is the number of words subsituted\n",
    "##Dw is the number of words deleted\n",
    "##Iw is the number of words inserted\n",
    "##Nw is the number of words in the reference\n",
    "def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
    "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case, delimiter)\n",
    "    if ref_len == 0:\n",
    "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
    "    wer = float(edit_distance) / ref_len\n",
    "    return wer\n",
    "##CER = (Sc + Dc + Ic) / Nc\n",
    "##Sc is the number of characters substituted\n",
    "##Dc is the number of characters deleted\n",
    "##Ic is the number of characters inserted\n",
    "##Nc is the number of characters in the reference\n",
    "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
    "    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case, remove_space)\n",
    "    if ref_len == 0:\n",
    "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
    "    cer = float(edit_distance) / ref_len\n",
    "    return cer\n",
    "##Decoder for Evaluation\n",
    "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        decodes.append(text_transform.int_to_text(decode))\n",
    "    return decodes, targets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Models' Structure and Layers\n",
    "\n",
    "First Model Design:\n",
    "    -The model will have two main neural network modules:\n",
    "        N layers of Residual Convolutional Neural Networks (ResCNN) to learn the relevant audio features. \n",
    "        M of Bidirectional Recurrent Neural Networks (BiRNN) to leverage the learned ResCNN audio features.\n",
    "    -The model is topped off with a fully connected layer used to classify characters per time step.\n",
    "    -The model outputs a probability matrix for characters which we'll use to feed into our decoder.\n",
    "    -Using a Viterbi Decoder to improve effieciency compared to a greedy decoder.\n",
    "Second Model Design:\n",
    "    -An LSTM based Sequence to Sequence model.\n",
    "    -Bidirectional Encoder & Attention Decoder followed by Beam Search to Improve the efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##First Model Implementation\n",
    "##First Module\n",
    "##CNN Layer Normalization\n",
    "class CNNLayerNorm(nn.Module):\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous() ##(batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous() ##(batch, channel, feature, time) \n",
    "\n",
    "##Residual CNN (The implementation is inspired from arXiv:1603.05027v3 [cs.CV] 25 Jul 2016)\n",
    "##We Normalize Layer by Layer instead of Batch Normalization\n",
    "class ResidualCNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "    def forward(self, x):\n",
    "        residual = x  ##(batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x ##(batch, channel, feature, time)\n",
    "\n",
    "##Second Module\n",
    "class BidirectionalGRU(nn.Module):\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "##Complete Model Structure The Number of CNNs and GRUs is variable and Accuracy increases as the increase with diminishing returns\n",
    "class SpeechRecognitionModel(nn.Module):  \n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_feats = n_feats//2\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  ##CNN for extracting features of interest\n",
    "        ##N Residual CNN Layers (Filter size = 32)\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, rnn_dim),  ##Bidirectional RNN returns rnn_dim*2\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  ##(batch, feature, time)\n",
    "        x = x.transpose(1, 2) ##(batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "##Second Model\n",
    "\n",
    "def attention_model(hidden_nodes = 256, emb_size = 256):\n",
    "    ##Attention Mechanism Layers\n",
    "    attn_repeat = RepeatVector(MAX_CHAR_SEQ_LEN)\n",
    "    attn_concat = Concatenate(axis=-1)\n",
    "    attn_dense1 = Dense(128, activation=\"tanh\")\n",
    "    attn_dense2 = Dense(1, activation=\"relu\")\n",
    "    attn_softmax = Lambda(lambda x: softmax(x,axis=1))\n",
    "    attn_dot = Dot(axes = 1)\n",
    "    def get_context(encoder_outputs, h_prev):\n",
    "        h_prev = attn_repeat(h_prev)\n",
    "        concat = attn_concat([encoder_outputs, h_prev])\n",
    "        e = attn_dense1(concat)\n",
    "        e = attn_dense2(e)\n",
    "        attention_weights = attn_softmax(e)\n",
    "        context = attn_dot([attention_weights, encoder_outputs])\n",
    "        return context\n",
    "    ##Encoder\n",
    "    char_inputs = Input(shape=(None,))\n",
    "    char_embedding_layer = Embedding(CHAR_TOKEN_COUNT, emb_size, input_length=MAX_CHAR_SEQ_LEN)\n",
    "    encoder = Bidirectional(LSTM(hidden_nodes, return_sequences=True, recurrent_dropout=0.2))\n",
    "    ##Decoder\n",
    "    decoder = LSTM(hidden_nodes, return_state=True, recurrent_dropout=0.2)\n",
    "    phone_embedding_layer = Embedding(PHONE_TOKEN_COUNT, emb_size)\n",
    "    embedding_reshaper = Reshape((1,emb_size,))\n",
    "    context_phone_concat = Concatenate(axis=-1)\n",
    "    context_phone_dense = Dense(hidden_nodes*3, activation=\"relu\")\n",
    "    output_layer = Dense(PHONE_TOKEN_COUNT, activation='softmax')\n",
    "    ##Training Encoder\n",
    "    char_embeddings = char_embedding_layer(char_inputs)\n",
    "    char_embeddings = Activation('relu')(char_embeddings)\n",
    "    char_embeddings = Dropout(0.5)(char_embeddings)\n",
    "    encoder_outputs = encoder(char_embeddings)\n",
    "    ##Training Decoder\n",
    "    h0 = Input(shape=(hidden_nodes,))\n",
    "    c0 = Input(shape=(hidden_nodes,))\n",
    "    h = h0 ##Hidden State\n",
    "    c = c0 #Cell State\n",
    "    phone_inputs = []\n",
    "    phone_outputs = []\n",
    "    for t in range(MAX_PHONE_SEQ_LEN):\n",
    "        phone_input = Input(shape=(None,))\n",
    "        phone_embeddings = phone_embedding_layer(phone_input)\n",
    "        phone_embeddings = Dropout(0.5)(phone_embeddings)\n",
    "        phone_embeddings = embedding_reshaper(phone_embeddings)\n",
    "        context = get_context(encoder_outputs, h)\n",
    "        phone_and_context = context_phone_concat([context, phone_embeddings])\n",
    "        phone_and_context = context_phone_dense(phone_and_context)\n",
    "        decoder_output, h, c = decoder(phone_and_context, initial_state = [h, c])\n",
    "        decoder_output = Dropout(0.5)(decoder_output)\n",
    "        phone_output = output_layer(decoder_output)\n",
    "        phone_inputs.append(phone_input)\n",
    "        phone_outputs.append(phone_output)\n",
    "    training_model = Model(inputs=[char_inputs, h0, c0] + phone_inputs, outputs=phone_outputs)\n",
    "   ##Testing Encoder\n",
    "    testing_encoder_model = Model(char_inputs, encoder_outputs)\n",
    "    ##Testing Decoder\n",
    "    test_prev_phone_input = Input(shape=(None,))\n",
    "    test_phone_embeddings = phone_embedding_layer(test_prev_phone_input)\n",
    "    test_phone_embeddings = embedding_reshaper(test_phone_embeddings)\n",
    "    test_h = Input(shape=(hidden_nodes,), name='test_h')\n",
    "    test_c = Input(shape=(hidden_nodes,), name='test_c')\n",
    "    test_encoding_input = Input(shape=(MAX_CHAR_SEQ_LEN, hidden_nodes*2,))\n",
    "    test_context = get_context(test_encoding_input, test_h)\n",
    "    test_phone_and_context = Concatenate(axis=-1)([test_context, test_phone_embeddings])\n",
    "    test_phone_and_context = context_phone_dense(test_phone_and_context)\n",
    "    test_seq, out_h, out_c = decoder(test_phone_and_context, initial_state = [test_h, test_c])\n",
    "    test_out = output_layer(test_seq)\n",
    "    testing_decoder_model = Model([test_prev_phone_input, test_h, test_c, test_encoding_input], [test_out,out_h,out_c])\n",
    "    return training_model, testing_encoder_model, testing_decoder_model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Helper Functions and Lists for Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_seq_matrix_decoder_output = np.pad(phone_seq_matrix,((0,0),(0,1),(0,0)), mode='constant')[:,1:,:]\n",
    "TEST_SIZE = 0.2\n",
    "(char_input_train, char_input_test, \n",
    " phone_input_train, phone_input_test, \n",
    " phone_output_train, phone_output_test) = train_test_split(\n",
    "    char_seq_matrix, phone_seq_matrix, phone_seq_matrix_decoder_output, \n",
    "    test_size=TEST_SIZE, random_state=42)\n",
    "TEST_EXAMPLE_COUNT = char_input_test.shape[0]\n",
    "##Helper Function for retrieving word from vector.\n",
    "def one_hot_matrix_to_word(char_seq):\n",
    "    word = ''\n",
    "    for char_vec in char_seq[0]:\n",
    "        if np.count_nonzero(char_vec) == 0:\n",
    "            break\n",
    "        hot_bit_idx = np.argmax(char_vec)\n",
    "        char = id_to_char[hot_bit_idx]\n",
    "        word += char\n",
    "    return word\n",
    "##Some words have multiple correct pronunciations.\n",
    "##If a prediction matches any alternative, consider it correct.\n",
    "def is_correct(word,test_pronunciation):\n",
    "    correct_pronuns = phonetic_dict[word]\n",
    "    for correct_pronun in correct_pronuns:\n",
    "        if test_pronunciation == correct_pronun:\n",
    "            return True\n",
    "    return False\n",
    "def syllable_count(phonetic_sp): \n",
    "    count = 0\n",
    "    for phone in phonetic_sp.split(): \n",
    "        if phone[-1].isdigit():\n",
    "            count += 1 \n",
    "    return count\n",
    "def is_syllable_count_correct(word, test_pronunciation):\n",
    "    correct_pronuns = phonetic_dict[word]\n",
    "    for correct_pronun in correct_pronuns:\n",
    "        if syllable_count(test_pronunciation) == syllable_count(correct_pronun):\n",
    "            return True\n",
    "    return False    \n",
    "def bleu_score(word,test_pronunciation):\n",
    "    references = [pronun.split() for pronun in phonetic_dict[word]]\n",
    "    smooth = SmoothingFunction().method1\n",
    "    return sentence_bleu(references, test_pronunciation.split(), smoothing_function=smooth)\n",
    "def evaluate(test_examples, encoder, decoder, word_decoder, predictor):\n",
    "    correct_syllable_counts = 0\n",
    "    perfect_predictions = 0\n",
    "    bleu_scores = []\n",
    "    for example_idx in range(TEST_EXAMPLE_COUNT):\n",
    "        example_char_seq = test_examples[example_idx:example_idx+1]\n",
    "        predicted_pronun = predictor(example_char_seq, encoder, decoder)\n",
    "        example_word = word_decoder(example_char_seq)\n",
    "        perfect_predictions += is_correct(example_word,predicted_pronun)\n",
    "        correct_syllable_counts += is_syllable_count_correct(example_word,predicted_pronun)\n",
    "        bleu = bleu_score(example_word,predicted_pronun)\n",
    "        bleu_scores.append(bleu)\n",
    "    syllable_acc = correct_syllable_counts / TEST_EXAMPLE_COUNT\n",
    "    perfect_acc = perfect_predictions / TEST_EXAMPLE_COUNT\n",
    "    avg_bleu_score = np.mean(bleu_scores)\n",
    "    return syllable_acc, perfect_acc, avg_bleu_score\n",
    "def print_results(model_name, syllable_acc, perfect_acc, avg_bleu_score):\n",
    "    print(model_name)\n",
    "    print('-'*20)\n",
    "    print('Syllable Accuracy: %s%%' % round(syllable_acc*100, 1))\n",
    "    print('Perfect Accuracy: %s%%' % round(perfect_acc*100, 1))\n",
    "    print('Bleu Score: %s' % round(avg_bleu_score, 4))\n",
    "def dataset_for_embeddings():\n",
    "    char_seqs = []\n",
    "    phone_seqs = []\n",
    "    for word,pronuns in phonetic_dict.items():\n",
    "        word_matrix = np.zeros((MAX_CHAR_SEQ_LEN))\n",
    "        for t,char in enumerate(word):\n",
    "            word_matrix[t] = char_to_id[char]\n",
    "        for pronun in pronuns:\n",
    "            pronun_matrix = np.zeros((MAX_PHONE_SEQ_LEN))\n",
    "            phones = [START_PHONE_SYM] + pronun.split() + [END_PHONE_SYM]\n",
    "            for t, phone in enumerate(phones):\n",
    "                pronun_matrix[t] = phone_to_id[phone]\n",
    "            char_seqs.append(word_matrix)\n",
    "            phone_seqs.append(pronun_matrix)\n",
    "    return np.array(char_seqs), np.array(phone_seqs)\n",
    "char_emb_matrix, phone_emb_matrix = dataset_for_embeddings()\n",
    "(emb_char_input_train, emb_char_input_test,emb_phone_input_train, emb_phone_input_test) = train_test_split(char_emb_matrix, phone_emb_matrix, test_size=TEST_SIZE, random_state=42)\n",
    "def id_vec_to_word(emb_char_seq):\n",
    "    word = ''\n",
    "    for char_id in emb_char_seq[0]:\n",
    "        char = id_to_char[char_id]\n",
    "        word += char\n",
    "    return word.strip()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Training of both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Training First Model\n",
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    for batch_idx, _data in enumerate(train_loader):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data \n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(spectrograms)  # (batch, time, n_class)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(spectrograms), data_len,100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "##Training\n",
    "train_url=\"train-other-500\"\n",
    "learning_rate = 5e-4\n",
    "batch_size = 10\n",
    "hparams = {\n",
    "        \"n_cnn_layers\": 3,\n",
    "        \"n_rnn_layers\": 5,\n",
    "        \"rnn_dim\": 512,\n",
    "        \"n_class\": 29,\n",
    "        \"n_feats\": 128,\n",
    "        \"stride\":2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": 10\n",
    "    }\n",
    "torch.manual_seed(7)\n",
    "device = torch.device(\"cpu\")\n",
    "if not os.path.isdir(\"./data\"):\n",
    "  os.makedirs(\"./data\")\n",
    "  train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
    "else:\n",
    "  train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=False)\n",
    "kwargs = {}\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                **kwargs)\n",
    "model = SpeechRecognitionModel(\n",
    "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "        ).to(device)\n",
    "if os.path.exists(\"Model/FinalModel.pt\"):\n",
    "    model.load_state_dict(torch.load(\"Model/FinalModel.pt\", map_location=torch.device('cpu')))\n",
    "else:\n",
    "    print(\"No old Weights.\")\n",
    "print(model)\n",
    "print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "criterion = nn.CTCLoss(blank=28).to(device)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                            steps_per_epoch=int(len(train_loader)),\n",
    "                                            epochs=hparams['epochs'],\n",
    "                                            anneal_strategy='linear')\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment)\n",
    "    if os.path.exists(\"Model/Model\"+str(epoch)+\".pt\"):\n",
    "        os.remove(\"Model/Model\"+str(epoch)+\".pt\")\n",
    "    print(\"Saving Model Weights.\")\n",
    "    torch.save(model.state_dict(), \"Model/Model\"+str(epoch)+\".pt\")\n",
    "if os.path.exists(\"Model/FinalModel.pt\"):\n",
    "    os.remove(\"Model/FinalModel.pt\")\n",
    "print(\"Saving Model Weights.\")\n",
    "torch.save(model.state_dict(), \"Model/FinalModel.pt\")\n",
    "\n",
    "##Training Second Model\n",
    "def train_attention(model, weights_path, validation_size=0.2, epochs=100):    \n",
    "    h0 = np.zeros((emb_char_input_train.shape[0], 256))\n",
    "    c0 = np.zeros((emb_char_input_train.shape[0], 256))\n",
    "    inputs = list(emb_phone_input_train.swapaxes(0,1))\n",
    "    outputs = list(phone_output_train.swapaxes(0,1))\n",
    "    callbacks = []\n",
    "    if validation_size > 0:\n",
    "        checkpointer = ModelCheckpoint(filepath=weights_path, verbose=1, save_best_only=True)\n",
    "        stopper = EarlyStopping(monitor='val_loss',patience=3)\n",
    "        callbacks = [checkpointer, stopper]\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    model.fit([emb_char_input_train, h0, c0] + inputs, outputs,\n",
    "              batch_size=256,\n",
    "              epochs=epochs,\n",
    "              validation_split=validation_size,\n",
    "              callbacks=callbacks)\n",
    "    if validation_size == 0:\n",
    "        model.save_weights(weights_path)\n",
    "FINAL_ATTENTION_MODEL_WEIGHTS = os.path.join('GRAPHEME_TO_PHONEME/input', 'predicting-english-pronunciations-model-weights', 'final_attention_model_weights.hdf5')\n",
    "attn_training_model, attn_testing_encoder_model, attn_testing_decoder_model = attention_model()\n",
    "train_attention(attn_training_model, FINAL_ATTENTION_MODEL_WEIGHTS, validation_size=0.0, epochs=29)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Evaluating both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Evaluating First Model with Greedy Decoder\n",
    "def test(model, device, test_loader, criterion):\n",
    "    print('\\nevaluating...')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "    with torch.no_grad():\n",
    "        for i, _data in enumerate(test_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data \n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            output = model(spectrograms)  ##(batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) ##(time, batch, n_class)\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "            for j in range(len(decoded_preds)):\n",
    "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "    avg_cer = sum(test_cer)/len(test_cer)\n",
    "    avg_wer = sum(test_wer)/len(test_wer)\n",
    "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
    "##Testing\n",
    "test_url=\"test-other\"\n",
    "learning_rate = 5e-4\n",
    "batch_size = 10\n",
    "hparams = {\n",
    "        \"n_cnn_layers\": 3,\n",
    "        \"n_rnn_layers\": 5,\n",
    "        \"rnn_dim\": 512,\n",
    "        \"n_class\": 29,\n",
    "        \"n_feats\": 128,\n",
    "        \"stride\":2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": 1\n",
    "    }\n",
    "torch.manual_seed(7)\n",
    "device = torch.device(\"cpu\")\n",
    "if not os.path.isdir(\"./data\"):\n",
    "  os.makedirs(\"./data\")\n",
    "  test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
    "else:\n",
    "  test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=False)\n",
    "kwargs = {}\n",
    "test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                **kwargs)\n",
    "model = SpeechRecognitionModel(\n",
    "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "        ).to(device)\n",
    "model.load_state_dict(torch.load(\"Model/FinalModel.pt\", map_location=torch.device('cpu')))\n",
    "optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "criterion = nn.CTCLoss(blank=28).to(device) \n",
    "test(model, device, test_loader, criterion)\n",
    "\n",
    "##Evaluating Second Model\n",
    "def predict_attention(input_char_seq, encoder, decoder):\n",
    "    encoder_outputs = encoder.predict(input_char_seq) \n",
    "    output_phone_seq = np.array([[phone_to_id[START_PHONE_SYM]]])\n",
    "    h = np.zeros((emb_char_input_train.shape[0], 256))\n",
    "    c = np.zeros((emb_char_input_train.shape[0], 256))\n",
    "    end_found = False \n",
    "    pronunciation = '' \n",
    "    while not end_found:\n",
    "        decoder_output, h, c = decoder.predict([output_phone_seq, h, c, encoder_outputs])\n",
    "        ##Greedy Prediction\n",
    "        predicted_phone_idx = np.argmax(decoder_output[0,:])\n",
    "        predicted_phone = id_to_phone[predicted_phone_idx]\n",
    "        pronunciation += predicted_phone + ' '\n",
    "        if predicted_phone == END_PHONE_SYM or len(pronunciation.split()) > MAX_PHONE_SEQ_LEN: \n",
    "            end_found = True\n",
    "        ##Prepare for next time step\n",
    "        output_phone_seq = np.array([[predicted_phone_idx]])\n",
    "    return pronunciation.strip()\n",
    "##BeamSearch Decoder\n",
    "def predict_beamsearch(input_char_seq, encoder, decoder, k=3):\n",
    "    a = encoder.predict(input_char_seq) \n",
    "    s = np.zeros((emb_char_input_train.shape[0], 256))\n",
    "    c = np.zeros((emb_char_input_train.shape[0], 256))\n",
    "    all_seqs = []\n",
    "    all_seq_scores = []\n",
    "    live_seqs = [[phone_to_id[START_PHONE_SYM]]]\n",
    "    live_scores = [0]\n",
    "    live_states = [[s,c]]\n",
    "    while len(live_seqs) > 0: \n",
    "        new_live_seqs = [] \n",
    "        new_live_scores = [] \n",
    "        new_live_states = []\n",
    "        for sidx,seq in enumerate(live_seqs):\n",
    "            target_seq = np.array([[seq[-1]]])\n",
    "            output_token_probs, s, c = decoder.predict([target_seq] + live_states[sidx] + [a])\n",
    "            best_token_indicies = output_token_probs[0,:].argsort()[-k:]\n",
    "            for token_index in best_token_indicies:\n",
    "                new_seq = seq + [token_index]\n",
    "                prob = output_token_probs[0,:][token_index]\n",
    "                new_seq_score = live_scores[sidx] - np.log(prob)\n",
    "                if id_to_phone[token_index] == END_PHONE_SYM or len(new_seq) > MAX_PHONE_SEQ_LEN:\n",
    "                    all_seqs.append(new_seq) \n",
    "                    all_seq_scores.append(new_seq_score) \n",
    "                    continue\n",
    "                new_live_seqs.append(new_seq)\n",
    "                new_live_scores.append(new_seq_score)\n",
    "                new_live_states.append([s, c])        \n",
    "        while len(new_live_scores) > k:\n",
    "            worst_seq_score_idx = np.array(new_live_scores).argsort()[-1] \n",
    "            del new_live_seqs[worst_seq_score_idx]\n",
    "            del new_live_scores[worst_seq_score_idx]\n",
    "            del new_live_states[worst_seq_score_idx]      \n",
    "        live_seqs = new_live_seqs\n",
    "        live_scores = new_live_scores\n",
    "        live_states = new_live_states\n",
    "    best_idx = np.argmin(all_seq_scores)\n",
    "    score = all_seq_scores[best_idx]\n",
    "    pronunciation = ''\n",
    "    for i in all_seqs[best_idx]:\n",
    "        pronunciation += id_to_phone[i] + ' '\n",
    "    return pronunciation.strip()\n",
    "FINAL_ATTENTION_MODEL_WEIGHTS = os.path.join('GRAPHEME_TO_PHONEME/input', 'predicting-english-pronunciations-model-weights', 'final_attention_model_weights.hdf5')\n",
    "attn_training_model, attn_testing_encoder_model, attn_testing_decoder_model = attention_model()\n",
    "attn_training_model.load_weights(FINAL_ATTENTION_MODEL_WEIGHTS)\n",
    "syllable_acc, perfect_acc, avg_bleu_score = evaluate(emb_char_input_test, attn_testing_encoder_model, attn_testing_decoder_model, id_vec_to_word, predict_beamsearch)\n",
    "print_results('Attention Model with BeamSearch Decoder', syllable_acc, perfect_acc, avg_bleu_score)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Prediction for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Prediction for first model\n",
    "def GreedyDecoderPredictor(output, blank_label=28, collapse_repeated=True):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        decodes.append(text_transform.int_to_text(decode))\n",
    "    return decodes\n",
    "def predict(model, device, audiofile):\n",
    "    print('\\nPredicting...')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        waveform, sample_rate = torchaudio.load(audiofile)\n",
    "        spectrogram=[]\n",
    "        spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        spectrogram.append(spec)\n",
    "        spectrogram=nn.utils.rnn.pad_sequence(spectrogram, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "        spectrogram=spectrogram.to(device)\n",
    "        output = model(spectrogram)  ##(batch, time, n_class)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        output = output.transpose(0, 1) ##(time, batch, n_class)\n",
    "        output=output.transpose(0, 1)\n",
    "        labels=\"\"\n",
    "        for x in range(len(text_transform.index_map)):\n",
    "            labels=labels+text_transform.index_map[x]\n",
    "        labels=labels+\"!\"\n",
    "        ImprovedPrediction,Ignore = viterbi_search(output[0].numpy(),labels) ##Viterbi Decoder\n",
    "        return str(ImprovedPrediction)\n",
    "def FinalPredict1(Audiofile):\n",
    "    sound = AudioSegment.from_mp3(\"TestAudio/Assessments/\"+Audiofile+\".mp3\")\n",
    "    sound = sound.set_channels(1)\n",
    "    sound.export(\"TestAudio/Noisy/Input.wav\", format=\"wav\")\n",
    "    x, sr = librosa.load(\"TestAudio/Noisy/Input.wav\", sr=16000)\n",
    "    sf.write(\"C:/Users/samer/Desktop/ASR/TestAudio/Noisy/Input.wav\", x, samplerate =16000, subtype ='PCM_16')\n",
    "    !python -m denoiser.enhance --master64 --noisy_dir C:\\Users\\samer\\Desktop\\ASR\\TestAudio\\Noisy --out_dir C:\\Users\\samer\\Desktop\\ASR\\TestAudio\\Clean\n",
    "    learning_rate = 5e-4\n",
    "    batch_size = 10\n",
    "    hparams = {\n",
    "        \"n_cnn_layers\": 3,\n",
    "        \"n_rnn_layers\": 5,\n",
    "        \"rnn_dim\": 512,\n",
    "        \"n_class\": 29,\n",
    "        \"n_feats\": 128,\n",
    "        \"stride\":2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": 10\n",
    "    }\n",
    "    torch.manual_seed(7)\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = SpeechRecognitionModel(\n",
    "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "        )\n",
    "    model.load_state_dict(torch.load(\"Model/FinalModel.pt\", map_location=torch.device('cpu')))\n",
    "    Answer=predict(model,device,\"TestAudio/Clean/Input_enhanced.wav\")\n",
    "    if(Answer[len(Answer)-1]=='s' and (Answer[len(Answer)-2]=='t' or Answer[len(Answer)-2]=='d')):\n",
    "        Answer=Answer[:-1]\n",
    "    Answer=Answer.replace(\" \", \"\")\n",
    "    Answer=Answer.replace(\"!\", \"\")\n",
    "    return Answer\n",
    "\n",
    "##Prediction for Second Model\n",
    "def predict_beamsearch(input_char_seq, encoder, decoder, k=3):\n",
    "    a = encoder.predict(input_char_seq) \n",
    "    s = np.zeros((emb_char_input_train.shape[0], 256))\n",
    "    c = np.zeros((emb_char_input_train.shape[0], 256))\n",
    "    all_seqs = []\n",
    "    all_seq_scores = []\n",
    "    live_seqs = [[phone_to_id[START_PHONE_SYM]]]\n",
    "    live_scores = [0]\n",
    "    live_states = [[s,c]]\n",
    "    while len(live_seqs) > 0: \n",
    "        new_live_seqs = [] \n",
    "        new_live_scores = [] \n",
    "        new_live_states = []\n",
    "        for sidx,seq in enumerate(live_seqs):\n",
    "            target_seq = np.array([[seq[-1]]])\n",
    "            output_token_probs, s, c = decoder.predict([target_seq] + live_states[sidx] + [a])\n",
    "            best_token_indicies = output_token_probs[0,:].argsort()[-k:]\n",
    "            for token_index in best_token_indicies:\n",
    "                new_seq = seq + [token_index]\n",
    "                prob = output_token_probs[0,:][token_index]\n",
    "                new_seq_score = live_scores[sidx] - np.log(prob)\n",
    "                if id_to_phone[token_index] == END_PHONE_SYM or len(new_seq) > MAX_PHONE_SEQ_LEN:\n",
    "                    all_seqs.append(new_seq) \n",
    "                    all_seq_scores.append(new_seq_score) \n",
    "                    continue\n",
    "                new_live_seqs.append(new_seq)\n",
    "                new_live_scores.append(new_seq_score)\n",
    "                new_live_states.append([s, c])        \n",
    "        while len(new_live_scores) > k:\n",
    "            worst_seq_score_idx = np.array(new_live_scores).argsort()[-1] \n",
    "            del new_live_seqs[worst_seq_score_idx]\n",
    "            del new_live_scores[worst_seq_score_idx]\n",
    "            del new_live_states[worst_seq_score_idx]      \n",
    "        live_seqs = new_live_seqs\n",
    "        live_scores = new_live_scores\n",
    "        live_states = new_live_states\n",
    "    best_idx = np.argmin(all_seq_scores)\n",
    "    score = all_seq_scores[best_idx]\n",
    "    pronunciation = ''\n",
    "    for i in all_seqs[best_idx]:\n",
    "        pronunciation += id_to_phone[i] + ' '\n",
    "    return pronunciation.strip()\n",
    "def FinalPredict2(TEXT):\n",
    "    FINAL_ATTENTION_MODEL_WEIGHTS = os.path.join('GRAPHEME_TO_PHONEME/input', 'predicting-english-pronunciations-model-weights', 'final_attention_model_weights.hdf5')\n",
    "    attn_training_model, attn_testing_encoder_model, attn_testing_decoder_model = attention_model()\n",
    "    attn_training_model.load_weights(FINAL_ATTENTION_MODEL_WEIGHTS)\n",
    "    char_seqs = []\n",
    "    word_matrix = np.zeros((MAX_CHAR_SEQ_LEN))\n",
    "    for t,char in enumerate(TEXT):\n",
    "        word_matrix[t] = char_to_id[char.upper()]\n",
    "    char_seqs.append(word_matrix)\n",
    "    word_matrix=np.array(char_seqs)\n",
    "    return predict_beamsearch(word_matrix,attn_testing_encoder_model, attn_testing_decoder_model)\n",
    "\n",
    "##Final Function Using both Models to Retrieve Spoken Phonemes and Number of Syllables in an Audio File\n",
    "def Final(AudioFile):\n",
    "    Grapheme=FinalPredict1(AudioFile)\n",
    "    Phonemes=FinalPredict2(Grapheme)\n",
    "    SyllableCount=syllable_count(Phonemes)\n",
    "    Result=[]\n",
    "    Result.append(Grapheme)\n",
    "    Result.append(Phonemes)\n",
    "    Result.append(SyllableCount)\n",
    "    return Result\n",
    "\n",
    "Final(\"scissors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
